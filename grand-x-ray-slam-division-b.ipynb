{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":113002,"databundleVersionId":13471427,"sourceType":"competition"},{"sourceId":257537682,"sourceType":"kernelVersion"},{"sourceId":259988149,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:37:58.419355Z","iopub.execute_input":"2025-09-04T16:37:58.419686Z","iopub.status.idle":"2025-09-04T16:40:46.686166Z","shell.execute_reply.started":"2025-09-04T16:37:58.419660Z","shell.execute_reply":"2025-09-04T16:40:46.684909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ“Š Exploratory Data Analysis (EDA)\n\nIn this section, we perform initial exploration of the dataset to understand its structure, completeness, and label distribution.  \n\n---\n\n### 1. Dataset Overview\nWe start by loading the training CSV file and previewing the first 10 rows.\n\n- **Total Images** â†’ Total number of X-ray images available.  \n- **Total Patients** â†’ Unique patient identifiers present in the dataset.  \n- **Total Studies** â†’ Number of unique studies conducted.  \n\nThis gives a quick sense of dataset size and coverage.\n\n---\n\n### 2. Handling Missing Values\n- **Age** â†’ Missing ages are imputed with the **median** age.  \n- **Sex** â†’ Missing sex values are filled with `\"Unknown\"`.  \n\nThis ensures no missing values remain in critical demographic columns.\n\n---\n\n### 3. Label Columns\nWe define 14 condition labels for classification:  \n\n- **No Finding**, **Lung Opacity**, **Support Devices**, **Atelectasis**,  \n- **Cardiomegaly**, **Pleural Effusion**, **Enlarged Cardiomediastinum**,  \n- **Edema**, **Consolidation**, **Pneumonia**, **Fracture**,  \n- **Lung Lesion**, **Pneumothorax**, **Pleural Other**.  \n\nFor each condition, we calculate:  \n- **Count** â†’ Number of images with the condition.  \n- **Percent (%)** â†’ Prevalence as a percentage of the dataset.  \n\nThis helps us understand class imbalance and disease prevalence.\n\n---\n\n### 4. Data Quality Checks\nTo ensure data integrity, we check for the following:\n\n- **Duplicate Images** â†’ Verifies if any images are repeated.  \n- **Duplicate Patients** â†’ Expected, since a patient can have multiple images.  \n- **Invalid Age Values** â†’ Counts number of negative ages (should be zero).  \n\n---\n\n### 5. Outputs\n- **Summary metrics** (images, patients, studies).  \n- **Prevalence table** showing condition counts and percentages.  \n- **Data quality reports** on duplicates and invalid values.  \n\nThis forms the foundation of our dataset understanding before moving into deeper analysis and modeling.\n","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/grand-xray-slam-division-b/train2.csv')\ntrain_df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:45:07.801737Z","iopub.execute_input":"2025-09-04T16:45:07.802517Z","iopub.status.idle":"2025-09-04T16:45:08.159498Z","shell.execute_reply.started":"2025-09-04T16:45:07.802481Z","shell.execute_reply":"2025-09-04T16:45:08.158432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:45:09.271840Z","iopub.execute_input":"2025-09-04T16:45:09.272119Z","iopub.status.idle":"2025-09-04T16:45:09.311164Z","shell.execute_reply.started":"2025-09-04T16:45:09.272099Z","shell.execute_reply":"2025-09-04T16:45:09.310136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Summarize key metrics\ntotal_images = len(train_df)\ntotal_patients = train_df['Patient_ID'].nunique()\ntotal_studies = train_df['Study'].nunique()\nprint(f\"Total Images: {total_images}\")\nprint(f\"Total Patients: {total_patients}\")\nprint(f\"Total Studies: {total_studies}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:45:09.452918Z","iopub.execute_input":"2025-09-04T16:45:09.453328Z","iopub.status.idle":"2025-09-04T16:45:09.464264Z","shell.execute_reply.started":"2025-09-04T16:45:09.453295Z","shell.execute_reply":"2025-09-04T16:45:09.463333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:45:09.555419Z","iopub.execute_input":"2025-09-04T16:45:09.555753Z","iopub.status.idle":"2025-09-04T16:45:09.593672Z","shell.execute_reply.started":"2025-09-04T16:45:09.555726Z","shell.execute_reply":"2025-09-04T16:45:09.592632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['Age'] = train_df['Age'].fillna(train_df['Age'].median())\ntrain_df['Sex'] = train_df['Sex'].fillna('Unknown')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:45:14.158572Z","iopub.execute_input":"2025-09-04T16:45:14.158900Z","iopub.status.idle":"2025-09-04T16:45:14.178706Z","shell.execute_reply.started":"2025-09-04T16:45:14.158875Z","shell.execute_reply":"2025-09-04T16:45:14.177288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:45:14.195088Z","iopub.execute_input":"2025-09-04T16:45:14.195453Z","iopub.status.idle":"2025-09-04T16:45:14.243662Z","shell.execute_reply.started":"2025-09-04T16:45:14.195411Z","shell.execute_reply":"2025-09-04T16:45:14.242499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the 14 condition columns\nlabel_columns = ['No Finding', 'Lung Opacity', 'Support Devices', 'Atelectasis',\n                 'Cardiomegaly', 'Pleural Effusion', 'Enlarged Cardiomediastinum',\n                 'Edema', 'Consolidation', 'Pneumonia', 'Fracture', 'Lung Lesion',\n                 'Pneumothorax', 'Pleural Other']\n\n# Calculate counts and percentages for each condition\nlabel_counts = train_df[label_columns].sum()\nlabel_percentages = (label_counts / total_images * 100).round(2)\nprevalence_df = pd.DataFrame({\n    'Condition': label_counts.index,\n    'Count': label_counts.values,\n    'Percent (%)': label_percentages.values\n})\n\n# Display prevalence table\nprint(\"Label Prevalence:\")\nprint(prevalence_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:45:14.245674Z","iopub.execute_input":"2025-09-04T16:45:14.245998Z","iopub.status.idle":"2025-09-04T16:45:14.267383Z","shell.execute_reply.started":"2025-09-04T16:45:14.245968Z","shell.execute_reply":"2025-09-04T16:45:14.265748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for duplicate Image_Names\nduplicate_images = train_df['Image_name'].duplicated().sum()\nprint(f\"Duplicated Image_Name entries: {duplicate_images}\")\n\n# Check for duplicate Patient_IDs (expected due to multiple images per patient)\nduplicate_patients = total_images - total_patients\nprint(f\"Duplicated Patient_ID entries: {duplicate_patients}\")\n\n# Check for invalid Age values\ninvalid_ages = train_df['Age'].dropna()\ninvalid_ages = invalid_ages[invalid_ages < 0].count()\nprint(f\"Invalid Age values (<0): {invalid_ages}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:45:16.450916Z","iopub.execute_input":"2025-09-04T16:45:16.451392Z","iopub.status.idle":"2025-09-04T16:45:16.476307Z","shell.execute_reply.started":"2025-09-04T16:45:16.451361Z","shell.execute_reply":"2025-09-04T16:45:16.474644Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ§© Trainâ€“Validation Split\n\nTo ensure robust evaluation, we carefully split the dataset into **training** and **validation** subsets.  \n\n---\n\n### 1. Why Grouped Splitting?\nA key challenge in medical imaging datasets is **patient-level data leakage**.  \nIf images from the same patient appear in both training and validation sets, the model may learn patient-specific features instead of generalizable disease patterns.  \n\nâœ… To prevent this, we use **GroupShuffleSplit** with `Patient_ID` as the grouping variable.  \nThis guarantees that all images from a single patient are restricted to **either training or validation**, never both.  \n\n---\n\n### 2. Split Details\n- **Split ratio** â†’ 80% training, 20% validation.  \n- **Random state** fixed at `42` for reproducibility.  \n- Splitting performed only once (`n_splits=1`).  \n\n---\n\n### 3. Dataset Sizes\nAfter the split:  \n- **Train set size** â†’ Number of images used for model training.  \n- **Validation set size** â†’ Number of images held out for unbiased performance evaluation.  \n\nThis ensures that the modelâ€™s validation accuracy reflects real-world generalization ability rather than memorization.\n\n---\n","metadata":{}},{"cell_type":"code","source":"import os, random\nfrom collections import Counter\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, datasets\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import GroupShuffleSplit\n\n# Important: split by Patient_ID so same patient never leaks into train+val\ngss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\ntrain_idx, val_idx = next(gss.split(train_df, groups=train_df[\"Patient_ID\"]))\n\ntrain_df_split = train_df.iloc[train_idx].reset_index(drop=True)\nval_df_split   = train_df.iloc[val_idx].reset_index(drop=True)\n\nprint(f\"Train size: {len(train_df_split)} | Val size: {len(val_df_split)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:45:18.869955Z","iopub.execute_input":"2025-09-04T16:45:18.870335Z","iopub.status.idle":"2025-09-04T16:45:22.678560Z","shell.execute_reply.started":"2025-09-04T16:45:18.870308Z","shell.execute_reply":"2025-09-04T16:45:22.677301Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ—‚ï¸ Custom Dataset Class: `ChestXrayDataset`\n\nTo train deep learning models on medical images, we define a **custom PyTorch Dataset** that handles loading images, applying transformations, and returning labels in a structured format.\n\n---\n\n### 1. Purpose\nThe dataset class:\n- Loads **X-ray images** directly from the dataset directory.  \n- Ensures consistent preprocessing (e.g., grayscale conversion, resizing, normalization).  \n- Returns both **image tensors** and their associated **multi-label targets**.  \n\nThis provides a clean pipeline for training and validation.\n\n---\n\n### 2. Key Components\n\n#### **Initialization (`__init__`)**\n- Accepts a dataframe (`dataframe`) containing metadata and image names.  \n- Stores image directory (`img_dir`) for locating files.  \n- Applies optional transformations (`transform`) for augmentation and normalization.  \n- Extracts label columns (`label_columns`) and stores them as a NumPy float32 array.  \n\n#### **Dataset Length (`__len__`)**\n- Returns the number of samples (rows in the dataframe).  \n\n#### **Fetching a Sample (`__getitem__`)**\n- Retrieves an image name and constructs its full path.  \n- Loads the image with **PIL** and converts it to **grayscale** (`\"L\"`), since chest X-rays are single-channel.  \n- Applies transformations if provided.  \n- Returns:\n  - `(image, labels)` â†’ when labels exist (train/validation).  \n  - `(image, img_name)` â†’ for test set (no labels).  \n\n---\n\n### 3. Advantages\n- ðŸ”„ **Reusability** â†’ Works for train, validation, and test splits.  \n- ðŸ©» **Medical image ready** â†’ Handles grayscale conversion properly.  \n- âš¡ **Integration** â†’ Compatible with PyTorch `DataLoader` for efficient batching and shuffling.  \n\nThis dataset class forms the backbone of our training pipeline, ensuring images and labels are consistently processed.\n","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\nimport os\n\nclass ChestXrayDataset(Dataset):\n    def __init__(self, dataframe, img_dir, transform=None, label_columns=None):\n        self.df = dataframe.reset_index(drop=True)   # store as self.df\n        self.img_dir = img_dir\n        self.transform = transform\n        self.label_columns = label_columns\n        \n        # Store labels for easy access\n        if self.label_columns is not None:\n            self.targets = self.df[self.label_columns].values.astype(np.float32)\n        else:\n            self.targets = None\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_name = self.df.loc[idx, \"Image_name\"]\n        img_path = os.path.join(self.img_dir, img_name)\n        \n        # Load image\n        image = Image.open(img_path).convert(\"L\")  # X-ray = grayscale\n        if self.transform:\n            image = self.transform(image)\n        \n        # Labels (for train/val only)\n        if self.targets is not None:\n            labels = torch.tensor(self.targets[idx], dtype=torch.float32)\n            return image, labels\n        else:\n            return image, img_name  # for test set\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:16:12.771183Z","iopub.execute_input":"2025-09-04T17:16:12.771978Z","iopub.status.idle":"2025-09-04T17:16:12.779968Z","shell.execute_reply.started":"2025-09-04T17:16:12.771945Z","shell.execute_reply":"2025-09-04T17:16:12.778922Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## âš™ï¸ Data Pipeline Setup\n\nNow that we have our custom dataset class, we define the **data preprocessing pipeline** and prepare dataloaders for training and validation.\n\n---\n\n### 1. Image Transformations\nWe use **torchvision transforms** to preprocess and augment images before feeding them into the model.\n\n- **Train Transformations**\n  - `Resize(224, 224)` â†’ standardize input size for CNNs.  \n  - `RandomHorizontalFlip()` â†’ simulates left/right orientation changes.  \n  - `RandomRotation(8Â°)` â†’ introduces slight rotational variation.  \n  - `ColorJitter()` â†’ adds minor brightness/contrast shifts.  \n  - `Grayscale(num_output_channels=3)` â†’ converts single-channel X-ray to **3-channel grayscale** (so pretrained CNNs like ResNet can process them).  \n  - `ToTensor()` â†’ converts to PyTorch tensor.  \n  - `Normalize(mean, std)` â†’ applies ImageNet normalization.  \n\n- **Validation Transformations**\n  - Only resizing, grayscale conversion, tensor conversion, and normalization.  \n  - âŒ No random augmentations â†’ ensures consistent validation results.\n\n---\n\n### 2. Dataset Creation\nWe wrap the preprocessed data into our custom `ChestXrayDataset`:\n\n- **`train_ds`** â†’ Training split with augmentations.  \n- **`val_ds`** â†’ Validation split with minimal preprocessing.  \n\nThis makes the dataset ready for PyTorch `DataLoader`.\n\n---\n\n### 3. Dataloaders\nWe create efficient data pipelines:\n\n- **`train_loader`**\n  - `batch_size = BATCH_SIZE`  \n  - `shuffle=True` â†’ ensures batches are randomized each epoch.  \n  - `num_workers=4` & `pin_memory=True` â†’ speed up data loading.  \n\n- **`val_loader`**\n  - `shuffle=False` â†’ validation set order is fixed.  \n  - Same parallelization optimizations as training.\n\n---\n\n### 4. Handling Class Imbalance\nChest X-ray datasets are **highly imbalanced** (e.g., \"No Finding\" dominates).  \nTo address this imbalance in **multi-label classification**:\n\n- Count positives (`pos_counts`) and negatives (`neg_counts`) for each condition.  \n- Compute **`pos_weight = neg_counts / pos_counts`**.  \n- This weight is passed to **`BCEWithLogitsLoss`**, giving rare diseases higher importance during training.  \n\n---\n\n### 5. Device Setup\n- Automatically detects **GPU (`cuda`)** if available, otherwise falls back to CPU.  \n- Prints device information (including GPU name when applicable).  \n\nâœ… This ensures efficient training and fair handling of imbalanced medical labels.\n","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torchvision import transforms\n\n# ---- Transforms ----\nmean = [0.485, 0.456, 0.406]\nstd  = [0.229, 0.224, 0.225]\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(8),\n    transforms.ColorJitter(brightness=0.08, contrast=0.08),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\n# ---- Datasets ----\ntrain_ds = ChestXrayDataset(train_df_split, \"path/to/images\", transform=train_transform, label_columns=label_columns)\nval_ds   = ChestXrayDataset(val_df_split, \"path/to/images\", transform=val_transform, label_columns=label_columns)\n\n# ---- Dataloaders ----\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\nval_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n\n# ---- Class imbalance for BCEWithLogitsLoss ----\npos_counts = train_df_split[label_columns].sum().values\nneg_counts = len(train_df_split) - pos_counts\npos_weight = torch.tensor(neg_counts / pos_counts, dtype=torch.float32)\n\n# ---- Device ----\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)\nif DEVICE.type == \"cuda\":\n    print(\"CUDA device:\", torch.cuda.get_device_name(0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:16:16.170968Z","iopub.execute_input":"2025-09-04T17:16:16.171320Z","iopub.status.idle":"2025-09-04T17:16:16.217427Z","shell.execute_reply.started":"2025-09-04T17:16:16.171297Z","shell.execute_reply":"2025-09-04T17:16:16.215912Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ‹ï¸ Model Training Pipeline\n\nWith the dataset and dataloaders ready, we now move to **model definition, training, and fine-tuning**.\n\n---\n\n### 1. Model Architecture\nWe use a **ResNet-50** backbone pretrained on ImageNet and replace its classification head with a custom multi-label head:\n\n- **Dropout** â†’ reduces overfitting.  \n- **Hidden layer + ReLU** â†’ adds non-linearity.  \n- **Final Linear layer** â†’ outputs predictions for **14 conditions**.  \n\nThis allows transfer learning while adapting the network to chest X-ray classification.\n\n---\n\n### 2. Loss Function\nWe use **`BCEWithLogitsLoss`** (binary cross-entropy with sigmoid), suitable for **multi-label classification**.  \n- Weighted with **`pos_weight`** to handle strong class imbalance.  \n- Ensures rare conditions contribute fairly to the gradient updates.\n\n---\n\n### 3. Training Strategy\nWe adopt a **two-stage training** process:\n\n#### **Stage 1 â€” Train Head Only**\n- Backbone (ResNet-50 convolutional layers) **frozen**.  \n- Only the new classification head is trained.  \n- Optimizer â†’ `Adam`, learning rate = **1e-3**.  \n- LR Scheduler â†’ `ReduceLROnPlateau` for adaptive learning rate adjustment.  \n\n#### **Stage 2 â€” Fine-Tune Backbone**\n- All layers **unfrozen** for full network training.  \n- Optimizer â†’ `AdamW`, learning rate = **1e-4** (smaller to avoid catastrophic forgetting).  \n- LR Scheduler â†’ same as Stage 1.  \n\nThis strategy stabilizes training and avoids destroying pretrained weights.\n\n---\n\n### 4. Training Loop\nWe define a reusable function **`run_epoch`**:\n\n- **Mode switching** â†’ `train()` for training, `eval()` for validation.  \n- **Mixed precision option (`use_amp`)** for faster training on GPUs.  \n- Tracks:\n  - **Loss** â†’ mean BCE loss across batches.  \n  - **AUC (ROC-AUC score)** â†’ primary evaluation metric for multi-label tasks.  \n\n---\n\n### 5. Monitoring & Logging\nFor each epoch, we log:  \n- Training and validation **loss**.  \n- Training and validation **AUC**.  \n- **Time per epoch** for efficiency tracking.  \n\nThis allows us to monitor model convergence and early stop if necessary.\n\n---\n\nâœ… With this setup, the model benefits from **transfer learning**, balanced training for rare conditions, and careful fine-tuning of the backbone for maximum performance.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport copy\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import roc_auc_score\nfrom torchvision import models\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ---- Hyperparameters ----\nHEAD_EPOCHS   = 5          # train classifier head first\nFT_EPOCHS     = 12         # then fine-tune backbone\nBATCH_SIZE    = 32         # don't leave None, set explicitly\nLR_HEAD       = 1e-3       # higher LR for head\nLR_FT         = 1e-4       # lower LR for fine-tuning\nWEIGHT_DECAY  = 1e-4       # regularization\nPATIENCE      = 3          # early stopping patience\nMIN_LR        = 1e-7       # minimum learning rate for scheduler\n\ndef build_model(num_classes=14, dropout=0.3):\n    model = models.resnet50(pretrained=True)\n    in_features = model.fc.in_features\n    \n    # Better head: hidden layer + ReLU + Dropout\n    model.fc = nn.Sequential(\n        nn.Dropout(dropout),\n        nn.Linear(in_features, in_features // 2),\n        nn.ReLU(),\n        nn.Dropout(dropout/2),\n        nn.Linear(in_features // 2, num_classes)\n    )\n    return model\n# Build model\nmodel = build_model(num_classes=14, dropout=0.3).to(DEVICE)\n# Loss with pos_weight for multi-label\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(DEVICE))  # pos_weight must match labels shape### LOSS\n\n# Stage 1: freeze backbone except final fc\nfor name, param in model.named_parameters():\n    param.requires_grad = False\nfor name, param in model.fc.named_parameters():\n    param.requires_grad = True\n\nopt_head = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_HEAD, weight_decay=WEIGHT_DECAY)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    opt_head, mode='min', factor=0.5, patience=PATIENCE, min_lr=MIN_LR\n)\n\ndef run_epoch(model, loader, optimizer=None, train=False, device=DEVICE, use_amp=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    losses = []\n    all_labels = []\n    all_probs = []\n    scaler = torch.cuda.amp.GradScaler() if use_amp and device.type == 'cuda' else None\n    loop = tqdm(loader, desc='Train' if train else 'EVAL', leave= False)\n    for imgs, labels in loop:\n        imgs = imgs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        with torch.set_grad_enabled(train):\n            if scaler:\n                with torch.cuda.amp.autocast():\n                    logits = model(imgs)\n                    loss = criterion(logits, labels)\n\n            else:\n                logits = model(imgs)\n                loss = criterion(logits, labels)\n            probs = torch.sigmoid(logits).detach().cpu().numpy()\n            all_probs.append(probs)\n            all_labels.append(labels.detach().cpu().numpy())\n\n            if train:\n                if scaler:\n                    scaler.scale(loss).backward()\n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n\n                else:\n                    loss.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n        losses.append(loss.item())\n        loop.set_postfix(loss=np.mean(losses))\n    all_probs = np.concatenate(all_probs)\n    all_labels = np.concatenate(all_labels)\n    try:\n        auc = roc_auc_score(all_labels, all_probs)\n    except ValueError:\n        auc = float('nan')\n\n    return np.mean(losses), auc\n\n### STAGE - 1 HEAD TRAINING\nfor epoch in range(1,HEAD_EPOCHS+1):\n    t0 = time.time()\n    train_loss, train_auc = run_epoch(model, train_loader, optimizer=opt_head, train=True, use_amp=False)\n    val_loss, val_auc = run_epoch(model, val_loader, train=False, use_amp=False)\n    scheduler.step(val_loss)\n    print(f\"Epoch {epoch}/{HEAD_EPOCHS}  train_loss={train_loss:.4f} train_auc={train_auc:.4f}  val_loss={val_loss:.4f} val_auc={val_auc:.4f}  time={(time.time()-t0):.1f}s\")\n\n## UNFREEZE BACKBONE AND FINE TUNEEEEEE\n\nfor param in model.parameters():\n    param.requires_grad = True\n\nopt_ft = optim.AdamW(model.parameters(), lr=LR_FT, weight_decay=WEIGHT_DECAY)\nscheduler_ft = optim.lr_scheduler.ReduceLROnPlateau(\n    opt_ft, mode='min', factor=0.5, patience=PATIENCE, min_lr=MIN_LR\n)\n\nfor epoch in range(1,FT_EPOCHS+1):\n    t0 = time.time()\n    train_loss, train_auc = run_epoch(model, train_loader, optimizer=opt_ft, train=True, use_amp=False)\n    val_loss, val_auc = run_epoch(model, val_loader, train=False, use_amp=False)\n    scheduler_ft.step(val_loss)\n    print(f\"FT Epoch {epoch}/{FT_EPOCHS}  train_loss={train_loss:.4f} train_auc={train_auc:.4f}  val_loss={val_loss:.4f} val_auc={val_auc:.4f}  time={(time.time()-t0):.1f}s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:20:24.484304Z","iopub.execute_input":"2025-09-04T17:20:24.484676Z","iopub.status.idle":"2025-09-04T17:20:24.491190Z","shell.execute_reply.started":"2025-09-04T17:20:24.484654Z","shell.execute_reply":"2025-09-04T17:20:24.489661Z"}},"outputs":[],"execution_count":null}]}